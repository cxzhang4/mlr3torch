---
title: "Lazy Tensor"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Lazy Tensor}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(ggplot2)
theme_set(theme_bw())
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette introduces the `lazy_tensor` class, which is a vector type that can be used to lazily represent torch tensors of arbitary dimensions.
The `lazy_tensor` type can be illustrated using the predefined `"lazy_iris"` task, which is similar to the `"iris"` task, but stores the 4 features in a single `lazy_tensor` column instead of four numeric columns.

```{r}
library(mlr3torch)

tsk_iris = tsk("lazy_iris")
tsk_iris
lt = tsk_iris$data(cols = "x")$x
head(lt)
```

We can convert the `lazy_tensor` column to a `torch_tensor` by calling `materialize()`.
By default, this will return a list of `torch_tensor`s.

```{r}
materialize(lt[1:2])
```

We can also convert the `lazy_tensor` to a single tensor by setting `rbind = TRUE`.

```{r}
materialize(lt[1:2], rbind = TRUE)
```

An important feature of the `lazy_tensor` type is that it can be preprocessed like other (non-lazy) datatypes.
Below, we standardize the data using its mean and standard deviation, which we calculate from the materialized tensor.

```{r}
lt_mat = materialize(lt, rbind = TRUE)
mus = torch_mean(lt_mat, dim = 1L)
sigmas = torch_std(lt_mat, dim = 1L)
po_scale = po("trafo_scale", mean = mus, sd = sigmas)

tsk_iris_preproc = po_scale$train(list(tsk_iris))[[1L]]
tsk_iris_preproc
```

Materializing the (lazily) preprocessed lazy tensor is equivalent to (eagerly) preprocessing the materialized tensor.

```{r}
lt_preproc = tsk_iris_preproc$data(cols = "x")$x
torch_equal(materialize(lt_preproc, rbind = TRUE), (lt_mat - mus) / sigmas)
```

We can combine the preprocessing with a simple multi layer perceptron to create a `GraphLearner`.

```{r}
glrn = as_learner(
  po_scale %>>% lrn("classif.mlp", batch_size = 150, epochs = 10)
)

glrn$train(tsk("iris"))
```


## Creating a Lazy Tensor

Every `lazy_tensor` is built on top of a `torch::dataset`, so we start by creating an dataset containing random data.
The only restriction that we impose on the dataset is that it must have a `.getitem` method that returns a list of named tensors.
In this case, the dataset returns a single tensor named `x`.
While the data is stored in-memory in this example, this is not necessary and the `$.getitem()` method can e.g. load images from disk.
For more information on how to create `torch::dataset`s, we recommend reading the [torch package documentation](https://torch.mlverse.org/).

```{r}
mydata = dataset(
  initialize = function() {
    self$x = runif(1000, -1, 1)
  },
  .getbatch = function(i) list(x = torch_tensor(self$x[i])$unsqueeze(2)),
  .length = function() 1000
)()
```

In order to create a `lazy_tensor` from `mydata`, we have to annotate the returned shapes of the dataset by passing a named list to `dataset_shapes`.
The first dimension must be `NA` as it is the batch dimension.
We can also set a shape to `NULL` to indicate that it is unknown (e.g. because it is variable).

```{r}
lt = as_lazy_tensor(mydata, dataset_shapes = list(x = c(NA, 1)))
lt
```

We can convert this vector to a `torch_tensor` just like before

```{r}
materialize(lt[1], rbind = TRUE)
```

Because we added no preprocessing, this is the same as calling the `$.getbatch()` method on `mydata` and selecting the element `x`.

```{r}
torch_equal(
  materialize(lt[1], rbind = TRUE),
  mydata$.getbatch(1)$x
)
```

We continue with creating an example task from `lt`.

```{r}
library(data.table)
x = mydata$x
y = 0.2 + 0.1 * x - 0.1 * x^2 - 0.3 * x^3 + 0.5 * x^4 + 0.5 * x^7 + 0.6 * x^11 +  rnorm(length(mydata)) * 0.1
dt = data.table(y = y, x = lt)
task_poly = as_task_regr(dt, target = "y", id = "poly")
```

```{r}
library(ggplot2)
ggplot(data = data.frame(x = x, y = y)) +
  geom_point(aes(x = x, y = y))
```

In the next section, we will create a custom `PipeOp` to fit a polynomial regression model.

## Custom Preprocessing

In order to create a custom preprocessing operator for a lazy tensor, we have to create a new `PipeOp` class.
To make this as convenient as possible, `mlr3torch` offters a `pipeop_preproc_torch()` function that we recommend using for this purpose.
Its most important arguments are:
* `id` - Used as the default identifier of the `PipeOp`
* `fn` - The preprocessing function
* `shapes_out` - A function that returns the shapes of the output tensors given the input shapes


```{r}
PipeOpPreprocTorchPoly = pipeop_preproc_torch("poly",
  fn = function(x, degrees) {
    torch_cat(lapply(degrees, function(d) torch_pow(x, d)), dim = 2L)
  },
  shapes_out = function(shapes_in, param_vals, task) {
    shape = shapes_in[[1L]]
    stopifnot(length(shape) == 2L)
    list(c(NA, length(param_vals$degrees)))
  }
)

po_poly = PipeOpPreprocTorchPoly$new()
po_poly$param_set$set_values(
  degrees = c(0, 1, 2, 3, 4, 7, 11),
  stages = "both"
)

lrn_poly = as_learner(
  po_poly %>>% lrn("regr.mlp", batch_size = 256, epochs = 100)
)

pred = lrn_poly$predict(task_poly)

dt = melt(data.table(
  truth = pred$truth,
  response = pred$response,
  x = x),
  id.vars = "x", measure.vars = c("truth", "response")
)
dt$variable = factor(dt$variable, levels = c("truth", "response"))


ggplot(data = dt) +
  geom_point(aes(x = x, y = value, color = variable))
```

## Digging Into Internals

TODO


